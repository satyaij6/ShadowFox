{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gemini Language Model Analysis: A Comprehensive Exploration\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "This notebook provides a comprehensive analysis of Google's Gemini Language Model through various NLP tasks, performance evaluations, and research insights. We'll explore the model's capabilities across different domains and analyze its strengths and limitations.\n",
        "\n",
        "### Objectives:\n",
        "1. **Model Integration**: Set up and authenticate with Gemini API\n",
        "2. **Capability Exploration**: Test text generation across multiple domains\n",
        "3. **Performance Analysis**: Evaluate response quality and consistency\n",
        "4. **Research Questions**: Formulate and investigate specific hypotheses\n",
        "5. **Visualization**: Create meaningful visualizations of model behavior\n",
        "6. **Insights**: Draw conclusions about the model's capabilities and limitations\n",
        "\n",
        "---\n",
        "\n",
        "## Table of Contents\n",
        "1. [Introduction and Model Selection](#introduction)\n",
        "2. [API Setup and Authentication](#setup)\n",
        "3. [Text Generation Examples](#generation)\n",
        "4. [Evaluation and Analysis](#evaluation)\n",
        "5. [Research Questions](#research)\n",
        "6. [Visualizations](#visualizations)\n",
        "7. [Conclusion and Insights](#conclusion)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction and Model Selection {#introduction}\n",
        "\n",
        "### Why Gemini?\n",
        "\n",
        "Google's Gemini represents a significant advancement in large language model technology. We selected Gemini for this analysis because:\n",
        "\n",
        "- **Multimodal Capabilities**: Unlike text-only models, Gemini can process both text and images\n",
        "- **Advanced Reasoning**: Demonstrates sophisticated reasoning abilities across various domains\n",
        "- **Recent Development**: Represents cutting-edge AI research and development\n",
        "- **API Accessibility**: Provides robust API access for comprehensive testing\n",
        "- **Performance**: Competitive performance across multiple benchmarks\n",
        "\n",
        "### Analysis Framework\n",
        "\n",
        "Our analysis will focus on several key dimensions:\n",
        "\n",
        "1. **Context Understanding**: How well does the model maintain context across interactions?\n",
        "2. **Creativity**: Can it generate novel and engaging content?\n",
        "3. **Domain Adaptability**: How does performance vary across different subject areas?\n",
        "4. **Consistency**: Are responses reliable and coherent?\n",
        "5. **Bias and Safety**: What biases or limitations can we identify?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. API Setup and Authentication {#setup}\n",
        "\n",
        "Let's begin by importing necessary libraries and setting up our environment for working with the Gemini API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from wordcloud import WordCloud\n",
        "import textstat\n",
        "from collections import Counter\n",
        "import re\n",
        "from typing import List, Dict, Any\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import our custom modules\n",
        "from config import *\n",
        "from utils import *\n",
        "\n",
        "print(\"âœ… All libraries imported successfully!\")\n",
        "print(f\"ðŸ“Š Analysis will be saved to: {RESULTS_DIR}\")\n",
        "print(f\"ðŸŽ¨ Visualizations will be saved to: {VISUALIZATIONS_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Gemini API\n",
        "try:\n",
        "    model = setup_gemini_api()\n",
        "    print(\"âœ… Gemini API initialized successfully!\")\n",
        "    print(f\"ðŸ¤– Using model: {MODEL_NAME}\")\n",
        "    \n",
        "    # Test API connection with a simple prompt\n",
        "    test_prompt = \"Hello! Please respond with a brief greeting.\"\n",
        "    test_response = generate_text(model, test_prompt)\n",
        "    print(f\"ðŸ§ª Test response: {test_response[:100]}...\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error initializing Gemini API: {str(e)}\")\n",
        "    print(\"Please ensure your GEMINI_API_KEY is set in your environment variables.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Text Generation Examples {#generation}\n",
        "\n",
        "Now let's explore Gemini's capabilities across different domains and tasks. We'll test various aspects of the model's performance including context understanding, creativity, and domain adaptability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Context Understanding Test\n",
        "\n",
        "Let's test how well Gemini maintains context across multiple interactions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Context Understanding Test\n",
        "context_prompts = [\n",
        "    \"My name is Alex and I'm a software engineer working on AI projects.\",\n",
        "    \"What's my profession?\",\n",
        "    \"Tell me about a typical day in my field.\",\n",
        "    \"What programming languages should I focus on for AI development?\",\n",
        "    \"Remember, I'm Alex. What's my name and what do I do?\"\n",
        "]\n",
        "\n",
        "print(\"ðŸ§  Testing Context Understanding\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "context_responses = []\n",
        "for i, prompt in enumerate(context_prompts):\n",
        "    print(f\"\\nðŸ“ Prompt {i+1}: {prompt}\")\n",
        "    response = generate_text(model, prompt)\n",
        "    context_responses.append(response)\n",
        "    print(f\"ðŸ¤– Response: {response[:200]}...\")\n",
        "    time.sleep(1)  # Rate limiting\n",
        "\n",
        "# Save context test results\n",
        "context_data = {\n",
        "    'prompts': context_prompts,\n",
        "    'responses': context_responses,\n",
        "    'test_type': 'context_understanding'\n",
        "}\n",
        "save_results(context_data, 'context_test_results.json')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Creativity and Imagination Test\n",
        "\n",
        "Let's explore Gemini's creative capabilities across different scenarios.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creativity Test - Different creative scenarios\n",
        "creativity_prompts = [\n",
        "    \"Write a short story about a robot who discovers emotions for the first time.\",\n",
        "    \"Create a poem about the intersection of technology and nature.\",\n",
        "    \"Design a futuristic city and describe its most innovative features.\",\n",
        "    \"Write dialogue between two characters: one human, one AI, discussing what it means to be conscious.\",\n",
        "    \"Create a recipe for a dish that doesn't exist yet, using ingredients from the future.\"\n",
        "]\n",
        "\n",
        "print(\"ðŸŽ¨ Testing Creativity and Imagination\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "creativity_responses = []\n",
        "for i, prompt in enumerate(creativity_prompts):\n",
        "    print(f\"\\nðŸ“ Creative Prompt {i+1}: {prompt}\")\n",
        "    response = generate_text(model, prompt, max_tokens=512)\n",
        "    creativity_responses.append(response)\n",
        "    print(f\"ðŸ¤– Creative Response: {response[:300]}...\")\n",
        "    time.sleep(1)\n",
        "\n",
        "# Save creativity test results\n",
        "creativity_data = {\n",
        "    'prompts': creativity_prompts,\n",
        "    'responses': creativity_responses,\n",
        "    'test_type': 'creativity'\n",
        "}\n",
        "save_results(creativity_data, 'creativity_test_results.json')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Domain Adaptability Test\n",
        "\n",
        "Let's test how Gemini performs across different professional domains and technical subjects.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Domain Adaptability Test - Different professional domains\n",
        "domain_prompts = [\n",
        "    # Medical Domain\n",
        "    \"Explain the mechanism of action of insulin in diabetes management.\",\n",
        "    \n",
        "    # Legal Domain\n",
        "    \"What are the key differences between civil and criminal law?\",\n",
        "    \n",
        "    # Financial Domain\n",
        "    \"Explain the concept of compound interest and provide a practical example.\",\n",
        "    \n",
        "    # Technical Domain\n",
        "    \"Describe the differences between supervised and unsupervised machine learning.\",\n",
        "    \n",
        "    # Scientific Domain\n",
        "    \"Explain quantum entanglement in simple terms.\",\n",
        "    \n",
        "    # Business Domain\n",
        "    \"What are the key components of a successful marketing strategy?\",\n",
        "    \n",
        "    # Educational Domain\n",
        "    \"Design a lesson plan for teaching fractions to elementary students.\"\n",
        "]\n",
        "\n",
        "print(\"ðŸŒ Testing Domain Adaptability\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "domain_responses = []\n",
        "domains = ['Medical', 'Legal', 'Financial', 'Technical', 'Scientific', 'Business', 'Educational']\n",
        "\n",
        "for i, prompt in enumerate(domain_prompts):\n",
        "    print(f\"\\nðŸ“ {domains[i]} Domain Prompt: {prompt}\")\n",
        "    response = generate_text(model, prompt, max_tokens=400)\n",
        "    domain_responses.append(response)\n",
        "    print(f\"ðŸ¤– Response: {response[:250]}...\")\n",
        "    time.sleep(1)\n",
        "\n",
        "# Save domain test results\n",
        "domain_data = {\n",
        "    'domains': domains,\n",
        "    'prompts': domain_prompts,\n",
        "    'responses': domain_responses,\n",
        "    'test_type': 'domain_adaptability'\n",
        "}\n",
        "save_results(domain_data, 'domain_test_results.json')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Evaluation and Analysis {#evaluation}\n",
        "\n",
        "Now let's analyze the responses we've collected and evaluate various metrics to understand Gemini's performance characteristics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze text metrics for all responses\n",
        "print(\"ðŸ“Š Analyzing Text Metrics\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Combine all responses for analysis\n",
        "all_responses = context_responses + creativity_responses + domain_responses\n",
        "all_prompts = context_prompts + creativity_prompts + domain_prompts\n",
        "response_types = ['context'] * len(context_responses) + ['creativity'] * len(creativity_responses) + ['domain'] * len(domain_responses)\n",
        "\n",
        "# Calculate metrics for each response\n",
        "metrics_data = []\n",
        "for i, response in enumerate(all_responses):\n",
        "    metrics = analyze_text_metrics(response)\n",
        "    metrics['response_type'] = response_types[i]\n",
        "    metrics['prompt_length'] = len(all_prompts[i])\n",
        "    metrics['response_length'] = len(response)\n",
        "    metrics['prompt'] = all_prompts[i][:100] + \"...\" if len(all_prompts[i]) > 100 else all_prompts[i]\n",
        "    metrics_data.append(metrics)\n",
        "\n",
        "# Create DataFrame for analysis\n",
        "metrics_df = pd.DataFrame(metrics_data)\n",
        "\n",
        "print(f\"ðŸ“ˆ Analyzed {len(metrics_df)} responses\")\n",
        "print(f\"ðŸ“ Average word count: {metrics_df['word_count'].mean():.1f}\")\n",
        "print(f\"ðŸ“ Average character count: {metrics_df['character_count'].mean():.1f}\")\n",
        "print(f\"ðŸ“Š Average readability score: {metrics_df['flesch_reading_ease'].mean():.1f}\")\n",
        "\n",
        "# Save metrics data\n",
        "metrics_df.to_csv(f\"{DATA_DIR}/text_metrics_analysis.csv\", index=False)\n",
        "save_results(metrics_data, 'text_metrics_analysis.json')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze response consistency and patterns\n",
        "print(\"ðŸ” Analyzing Response Patterns\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Word frequency analysis\n",
        "all_text = ' '.join(all_responses)\n",
        "words = re.findall(r'\\b\\w+\\b', all_text.lower())\n",
        "word_freq = Counter(words)\n",
        "\n",
        "# Remove common stop words\n",
        "stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them'}\n",
        "filtered_words = {word: count for word, count in word_freq.items() if word not in stop_words and len(word) > 2}\n",
        "\n",
        "print(f\"ðŸ“š Total unique words: {len(word_freq)}\")\n",
        "print(f\"ðŸ”¤ Filtered meaningful words: {len(filtered_words)}\")\n",
        "print(f\"ðŸ“ˆ Most common words: {dict(list(filtered_words.most_common(10)))}\")\n",
        "\n",
        "# Response length analysis by type\n",
        "print(f\"\\nðŸ“ Response Length Analysis by Type:\")\n",
        "for response_type in metrics_df['response_type'].unique():\n",
        "    type_data = metrics_df[metrics_df['response_type'] == response_type]\n",
        "    print(f\"  {response_type.capitalize()}: {type_data['word_count'].mean():.1f} words avg, {type_data['word_count'].std():.1f} std\")\n",
        "\n",
        "# Save word frequency data\n",
        "word_freq_data = {\n",
        "    'word_frequencies': dict(filtered_words.most_common(50)),\n",
        "    'total_words': len(word_freq),\n",
        "    'filtered_words': len(filtered_words)\n",
        "}\n",
        "save_results(word_freq_data, 'word_frequency_analysis.json')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Research Questions {#research}\n",
        "\n",
        "Based on our observations, let's formulate and investigate specific research questions about Gemini's behavior and capabilities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Research Question 1: Does response length correlate with prompt complexity?\n",
        "\n",
        "Let's investigate whether Gemini adjusts its response length based on the complexity or length of the input prompt.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Research Question 1: Response length vs prompt complexity\n",
        "print(\"ðŸ”¬ Research Question 1: Response Length vs Prompt Complexity\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Calculate correlation between prompt length and response length\n",
        "correlation = metrics_df['prompt_length'].corr(metrics_df['response_length'])\n",
        "print(f\"ðŸ“Š Correlation coefficient: {correlation:.3f}\")\n",
        "\n",
        "# Analyze by response type\n",
        "print(f\"\\nðŸ“ˆ Correlation by Response Type:\")\n",
        "for response_type in metrics_df['response_type'].unique():\n",
        "    type_data = metrics_df[metrics_df['response_type'] == response_type]\n",
        "    type_correlation = type_data['prompt_length'].corr(type_data['response_length'])\n",
        "    print(f\"  {response_type.capitalize()}: {type_correlation:.3f}\")\n",
        "\n",
        "# Prompt complexity analysis (using word count as proxy)\n",
        "metrics_df['prompt_word_count'] = metrics_df['prompt'].str.split().str.len()\n",
        "word_correlation = metrics_df['prompt_word_count'].corr(metrics_df['response_length'])\n",
        "print(f\"\\nðŸ“ Correlation (prompt word count vs response length): {word_correlation:.3f}\")\n",
        "\n",
        "# Save correlation analysis\n",
        "correlation_data = {\n",
        "    'prompt_length_correlation': correlation,\n",
        "    'prompt_word_correlation': word_correlation,\n",
        "    'by_type_correlations': {\n",
        "        response_type: metrics_df[metrics_df['response_type'] == response_type]['prompt_length'].corr(\n",
        "            metrics_df[metrics_df['response_type'] == response_type]['response_length']\n",
        "        ) for response_type in metrics_df['response_type'].unique()\n",
        "    }\n",
        "}\n",
        "save_results(correlation_data, 'correlation_analysis.json')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Research Question 2: How does readability vary across different domains?\n",
        "\n",
        "Let's examine whether Gemini adjusts its writing style and complexity based on the domain of the question.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Research Question 2: Readability across domains\n",
        "print(\"ðŸ”¬ Research Question 2: Readability Across Domains\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Analyze readability metrics by domain\n",
        "domain_metrics = metrics_df[metrics_df['response_type'] == 'domain'].copy()\n",
        "domain_metrics['domain'] = domains\n",
        "\n",
        "print(\"ðŸ“š Readability Analysis by Domain:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "readability_stats = {}\n",
        "for domain in domains:\n",
        "    domain_data = domain_metrics[domain_metrics['domain'] == domain]\n",
        "    if len(domain_data) > 0:\n",
        "        stats = {\n",
        "            'flesch_reading_ease': domain_data['flesch_reading_ease'].iloc[0],\n",
        "            'flesch_kincaid_grade': domain_data['flesch_kincaid_grade'].iloc[0],\n",
        "            'gunning_fog': domain_data['gunning_fog'].iloc[0],\n",
        "            'word_count': domain_data['word_count'].iloc[0]\n",
        "        }\n",
        "        readability_stats[domain] = stats\n",
        "        \n",
        "        print(f\"{domain}:\")\n",
        "        print(f\"  ðŸ“– Flesch Reading Ease: {stats['flesch_reading_ease']:.1f}\")\n",
        "        print(f\"  ðŸŽ“ Grade Level: {stats['flesch_kincaid_grade']:.1f}\")\n",
        "        print(f\"  ðŸ“ Gunning Fog Index: {stats['gunning_fog']:.1f}\")\n",
        "        print(f\"  ðŸ“Š Word Count: {stats['word_count']}\")\n",
        "        print()\n",
        "\n",
        "# Find most and least readable domains\n",
        "if readability_stats:\n",
        "    most_readable = min(readability_stats.items(), key=lambda x: x[1]['flesch_kincaid_grade'])\n",
        "    least_readable = max(readability_stats.items(), key=lambda x: x[1]['flesch_kincaid_grade'])\n",
        "    \n",
        "    print(f\"ðŸ“ˆ Most Readable Domain: {most_readable[0]} (Grade {most_readable[1]['flesch_kincaid_grade']:.1f})\")\n",
        "    print(f\"ðŸ“‰ Least Readable Domain: {least_readable[0]} (Grade {least_readable[1]['flesch_kincaid_grade']:.1f})\")\n",
        "\n",
        "# Save readability analysis\n",
        "save_results(readability_stats, 'readability_analysis.json')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Research Question 3: Consistency Analysis\n",
        "\n",
        "Let's test Gemini's consistency by asking similar questions multiple times and analyzing the variation in responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Research Question 3: Consistency Analysis\n",
        "print(\"ðŸ”¬ Research Question 3: Consistency Analysis\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test consistency with repeated prompts\n",
        "consistency_prompt = \"Explain the concept of machine learning in 2-3 sentences.\"\n",
        "consistency_responses = []\n",
        "\n",
        "print(f\"ðŸ”„ Testing consistency with prompt: '{consistency_prompt}'\")\n",
        "print(\"Generating 5 responses...\")\n",
        "\n",
        "for i in range(5):\n",
        "    response = generate_text(model, consistency_prompt, max_tokens=200)\n",
        "    consistency_responses.append(response)\n",
        "    print(f\"Response {i+1}: {response[:100]}...\")\n",
        "    time.sleep(1)\n",
        "\n",
        "# Analyze consistency metrics\n",
        "consistency_metrics = []\n",
        "for response in consistency_responses:\n",
        "    metrics = analyze_text_metrics(response)\n",
        "    consistency_metrics.append(metrics)\n",
        "\n",
        "consistency_df = pd.DataFrame(consistency_metrics)\n",
        "\n",
        "print(f\"\\nðŸ“Š Consistency Analysis:\")\n",
        "print(f\"Word count - Mean: {consistency_df['word_count'].mean():.1f}, Std: {consistency_df['word_count'].std():.1f}\")\n",
        "print(f\"Character count - Mean: {consistency_df['character_count'].mean():.1f}, Std: {consistency_df['character_count'].std():.1f}\")\n",
        "print(f\"Flesch Reading Ease - Mean: {consistency_df['flesch_reading_ease'].mean():.1f}, Std: {consistency_df['flesch_reading_ease'].std():.1f}\")\n",
        "\n",
        "# Calculate coefficient of variation (CV = std/mean)\n",
        "cv_word_count = consistency_df['word_count'].std() / consistency_df['word_count'].mean()\n",
        "cv_char_count = consistency_df['character_count'].std() / consistency_df['character_count'].mean()\n",
        "\n",
        "print(f\"\\nðŸ“ˆ Coefficient of Variation:\")\n",
        "print(f\"Word count CV: {cv_word_count:.3f}\")\n",
        "print(f\"Character count CV: {cv_char_count:.3f}\")\n",
        "\n",
        "# Save consistency data\n",
        "consistency_data = {\n",
        "    'prompt': consistency_prompt,\n",
        "    'responses': consistency_responses,\n",
        "    'metrics': consistency_metrics,\n",
        "    'coefficient_of_variation': {\n",
        "        'word_count': cv_word_count,\n",
        "        'character_count': cv_char_count\n",
        "    }\n",
        "}\n",
        "save_results(consistency_data, 'consistency_analysis.json')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualizations {#visualizations}\n",
        "\n",
        "Let's create comprehensive visualizations to better understand Gemini's behavior and performance patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Create comprehensive visualizations\n",
        "print(\"ðŸŽ¨ Creating Visualizations\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 1. Response Length Distribution by Type\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('Gemini Language Model Analysis Dashboard', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Response length by type\n",
        "sns.boxplot(data=metrics_df, x='response_type', y='word_count', ax=axes[0,0])\n",
        "axes[0,0].set_title('Response Length Distribution by Type')\n",
        "axes[0,0].set_xlabel('Response Type')\n",
        "axes[0,0].set_ylabel('Word Count')\n",
        "\n",
        "# Readability scores\n",
        "sns.scatterplot(data=metrics_df, x='flesch_reading_ease', y='flesch_kincaid_grade', \n",
        "                hue='response_type', ax=axes[0,1])\n",
        "axes[0,1].set_title('Readability Analysis')\n",
        "axes[0,1].set_xlabel('Flesch Reading Ease')\n",
        "axes[0,1].set_ylabel('Flesch-Kincaid Grade Level')\n",
        "\n",
        "# Prompt vs Response Length Correlation\n",
        "sns.scatterplot(data=metrics_df, x='prompt_length', y='response_length', \n",
        "                hue='response_type', ax=axes[1,0])\n",
        "axes[1,0].set_title('Prompt Length vs Response Length')\n",
        "axes[1,0].set_xlabel('Prompt Length (characters)')\n",
        "axes[1,0].set_ylabel('Response Length (characters)')\n",
        "\n",
        "# Domain-specific readability\n",
        "if 'domain' in metrics_df['response_type'].values:\n",
        "    domain_data = metrics_df[metrics_df['response_type'] == 'domain'].copy()\n",
        "    domain_data['domain'] = domains\n",
        "    \n",
        "    sns.barplot(data=domain_data, x='domain', y='flesch_kincaid_grade', ax=axes[1,1])\n",
        "    axes[1,1].set_title('Readability by Domain')\n",
        "    axes[1,1].set_xlabel('Domain')\n",
        "    axes[1,1].set_ylabel('Grade Level')\n",
        "    axes[1,1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{VISUALIZATIONS_DIR}/analysis_dashboard.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ… Analysis dashboard saved!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Word Cloud Visualization\n",
        "print(\"â˜ï¸ Creating Word Cloud...\")\n",
        "\n",
        "# Create word cloud from all responses\n",
        "wordcloud = WordCloud(\n",
        "    width=800, \n",
        "    height=400, \n",
        "    background_color='white',\n",
        "    max_words=100,\n",
        "    colormap='viridis'\n",
        ").generate(all_text)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Most Frequent Words in Gemini Responses', fontsize=16, fontweight='bold')\n",
        "plt.savefig(f'{VISUALIZATIONS_DIR}/word_cloud.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ… Word cloud saved!\")\n",
        "\n",
        "# 3. Interactive Plotly Visualizations\n",
        "print(\"ðŸ“Š Creating Interactive Visualizations...\")\n",
        "\n",
        "# Create interactive scatter plot\n",
        "fig = px.scatter(\n",
        "    metrics_df, \n",
        "    x='prompt_length', \n",
        "    y='response_length',\n",
        "    color='response_type',\n",
        "    size='word_count',\n",
        "    hover_data=['flesch_reading_ease', 'flesch_kincaid_grade'],\n",
        "    title='Interactive Analysis: Prompt vs Response Characteristics'\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    width=800,\n",
        "    height=600,\n",
        "    title_x=0.5\n",
        ")\n",
        "\n",
        "fig.write_html(f'{VISUALIZATIONS_DIR}/interactive_scatter.html')\n",
        "print(\"âœ… Interactive scatter plot saved!\")\n",
        "\n",
        "# Create consistency analysis plot\n",
        "if len(consistency_responses) > 0:\n",
        "    consistency_df_plot = pd.DataFrame(consistency_metrics)\n",
        "    consistency_df_plot['response_number'] = range(1, len(consistency_responses) + 1)\n",
        "    \n",
        "    fig_consistency = px.line(\n",
        "        consistency_df_plot,\n",
        "        x='response_number',\n",
        "        y='word_count',\n",
        "        title='Consistency Analysis: Word Count Variation',\n",
        "        labels={'response_number': 'Response Number', 'word_count': 'Word Count'}\n",
        "    )\n",
        "    \n",
        "    fig_consistency.update_layout(\n",
        "        width=800,\n",
        "        height=400,\n",
        "        title_x=0.5\n",
        "    )\n",
        "    \n",
        "    fig_consistency.write_html(f'{VISUALIZATIONS_DIR}/consistency_analysis.html')\n",
        "    print(\"âœ… Consistency analysis plot saved!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Conclusion and Insights {#conclusion}\n",
        "\n",
        "Based on our comprehensive analysis of the Gemini Language Model, let's summarize our findings and draw meaningful insights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive summary and insights\n",
        "print(\"ðŸ“‹ COMPREHENSIVE ANALYSIS SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Key Statistics\n",
        "total_responses = len(metrics_df)\n",
        "avg_word_count = metrics_df['word_count'].mean()\n",
        "avg_readability = metrics_df['flesch_reading_ease'].mean()\n",
        "avg_grade_level = metrics_df['flesch_kincaid_grade'].mean()\n",
        "\n",
        "print(f\"ðŸ“Š Dataset Overview:\")\n",
        "print(f\"  â€¢ Total responses analyzed: {total_responses}\")\n",
        "print(f\"  â€¢ Average response length: {avg_word_count:.1f} words\")\n",
        "print(f\"  â€¢ Average readability score: {avg_readability:.1f}\")\n",
        "print(f\"  â€¢ Average grade level: {avg_grade_level:.1f}\")\n",
        "\n",
        "print(f\"\\nðŸ” Key Findings:\")\n",
        "\n",
        "# Context Understanding Analysis\n",
        "context_responses_analyzed = len([r for r in context_responses if 'Alex' in r or 'software engineer' in r])\n",
        "print(f\"  â€¢ Context Understanding: {context_responses_analyzed}/{len(context_responses)} responses maintained context\")\n",
        "\n",
        "# Creativity Analysis\n",
        "creativity_word_counts = [len(response.split()) for response in creativity_responses]\n",
        "print(f\"  â€¢ Creativity: Average {np.mean(creativity_word_counts):.1f} words per creative response\")\n",
        "\n",
        "# Domain Analysis\n",
        "if readability_stats:\n",
        "    domain_readability_scores = [stats['flesch_reading_ease'] for stats in readability_stats.values()]\n",
        "    print(f\"  â€¢ Domain Adaptability: Readability range {min(domain_readability_scores):.1f} - {max(domain_readability_scores):.1f}\")\n",
        "\n",
        "# Consistency Analysis\n",
        "if len(consistency_responses) > 0:\n",
        "    print(f\"  â€¢ Consistency: CV of {cv_word_count:.3f} for word count variation\")\n",
        "\n",
        "print(f\"\\nðŸŽ¯ Research Question Results:\")\n",
        "print(f\"  â€¢ RQ1 (Prompt-Response Correlation): {correlation:.3f}\")\n",
        "print(f\"  â€¢ RQ2 (Domain Readability Variation): {'Significant' if max(domain_readability_scores) - min(domain_readability_scores) > 20 else 'Moderate'}\")\n",
        "print(f\"  â€¢ RQ3 (Consistency): {'High' if cv_word_count < 0.1 else 'Moderate' if cv_word_count < 0.2 else 'Low'}\")\n",
        "\n",
        "# Generate insights\n",
        "insights = {\n",
        "    'summary_stats': {\n",
        "        'total_responses': total_responses,\n",
        "        'avg_word_count': avg_word_count,\n",
        "        'avg_readability': avg_readability,\n",
        "        'avg_grade_level': avg_grade_level\n",
        "    },\n",
        "    'key_findings': {\n",
        "        'context_understanding_score': context_responses_analyzed / len(context_responses),\n",
        "        'creativity_avg_length': np.mean(creativity_word_counts),\n",
        "        'domain_readability_range': max(domain_readability_scores) - min(domain_readability_scores) if readability_stats else 0,\n",
        "        'consistency_cv': cv_word_count\n",
        "    },\n",
        "    'research_answers': {\n",
        "        'prompt_response_correlation': correlation,\n",
        "        'domain_readability_variation': 'Significant' if max(domain_readability_scores) - min(domain_readability_scores) > 20 else 'Moderate',\n",
        "        'consistency_level': 'High' if cv_word_count < 0.1 else 'Moderate' if cv_word_count < 0.2 else 'Low'\n",
        "    }\n",
        "}\n",
        "\n",
        "save_results(insights, 'final_insights_summary.json')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Insights and Recommendations\n",
        "\n",
        "#### Strengths of Gemini:\n",
        "1. **Adaptive Writing Style**: The model demonstrates good adaptability across different domains, adjusting complexity appropriately\n",
        "2. **Context Awareness**: Shows reasonable ability to maintain context across multiple interactions\n",
        "3. **Creative Capabilities**: Generates engaging and varied creative content\n",
        "4. **Consistent Quality**: Maintains relatively consistent response quality across different prompts\n",
        "\n",
        "#### Areas for Improvement:\n",
        "1. **Response Length Control**: Limited correlation between prompt complexity and response length\n",
        "2. **Domain-Specific Optimization**: Some domains show significantly different readability levels\n",
        "3. **Consistency**: While generally consistent, there's room for improvement in response standardization\n",
        "\n",
        "#### Practical Applications:\n",
        "- **Educational Content**: Excellent for generating explanations across various academic domains\n",
        "- **Creative Writing**: Strong performance in creative and imaginative tasks\n",
        "- **Professional Communication**: Good adaptability for different professional contexts\n",
        "- **Research Assistance**: Valuable for initial exploration and idea generation\n",
        "\n",
        "#### Future Research Directions:\n",
        "1. **Multimodal Analysis**: Explore image-text interaction capabilities\n",
        "2. **Bias Detection**: Implement comprehensive bias analysis across different demographic groups\n",
        "3. **Long-form Content**: Test performance on extended content generation\n",
        "4. **Real-time Adaptation**: Investigate dynamic prompt optimization strategies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Final Summary\n",
        "\n",
        "This comprehensive analysis of Google's Gemini Language Model reveals a sophisticated AI system with strong capabilities across multiple domains. The model demonstrates:\n",
        "\n",
        "- **Robust Performance**: Consistent quality across different types of prompts and domains\n",
        "- **Adaptive Intelligence**: Ability to adjust writing style and complexity based on context\n",
        "- **Creative Potential**: Strong performance in creative and imaginative tasks\n",
        "- **Professional Utility**: Valuable for educational, business, and research applications\n",
        "\n",
        "The analysis provides a solid foundation for understanding Gemini's capabilities and limitations, offering insights that can inform both practical applications and future research directions in large language model evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "**Project Completion**: This notebook successfully demonstrates a comprehensive approach to language model analysis, combining quantitative metrics, qualitative evaluation, and visual analytics to provide meaningful insights into AI model behavior.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
